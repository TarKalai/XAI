{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "05c5dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\XAI\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from inseq import load_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 1. Load model with gradient-based attribution\n",
    "model = load_model(\"gpt2\", \"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = model.model.get_input_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1da859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_toward_target(input_text, target_token) : \n",
    "\n",
    "    # 3. Get token IDs\n",
    "    inputs = model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "    target_id = model.tokenizer.convert_tokens_to_ids(target_token)\n",
    "\n",
    "    # # For optimizing toward a phrase:\n",
    "    # target_ids = [model.tokenizer.convert_tokens_to_ids(t) for t in [\"lazy\", \"dog\"]]\n",
    "    # target_logits = sum(outputs.logits[0, -len(target_ids):, target_ids].diag())\n",
    "\n",
    "    if target_id == model.tokenizer.unk_token_id:\n",
    "        print(f\"Warning: Target token '{target_token}' not found in vocabulary. Using UNK ID ({target_id}). Results might not be meaningful.\")\n",
    "        return None\n",
    "\n",
    "    # 4. Perform forward pass with gradient tracking\n",
    "    with torch.set_grad_enabled(True):\n",
    "        # Get embeddings and mark as requiring gradients\n",
    "        embeddings = embedding_layer(inputs.input_ids)\n",
    "        embeddings.retain_grad()\n",
    "        embeddings.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass through model\n",
    "        outputs = model.model(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get logits for the target position (next token prediction)\n",
    "        target_position = -1  # Position after last input token\n",
    "        target_logit = outputs.logits[0, target_position, target_id]\n",
    "        \n",
    "        # Compute gradients\n",
    "        target_logit.backward()\n",
    "        gradients = embeddings.grad.clone()\n",
    "\n",
    "    # # 5. Modify embeddings with gradients\n",
    "    # print(\"Comuted Gradients: \", gradients)\n",
    "    modified_embeddings = embeddings + gradients*4\n",
    "\n",
    "    all_embeddings = embedding_layer.weight #Vocab Embeddings\n",
    "    new_token_ids = []\n",
    "    for i, emb in enumerate(modified_embeddings[0]) : \n",
    "        distances = F.cosine_similarity(emb, all_embeddings, dim=-1)\n",
    "        new_id = torch.argmax(distances).item()\n",
    "        new_token_ids.append(new_id)\n",
    "\n",
    "    #Decode the new tokens\n",
    "    new_tokens = model.tokenizer.convert_ids_to_tokens(new_token_ids)\n",
    "    new_text = model.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "    print(f\"Modified text: {new_text}\")\n",
    "\n",
    "\n",
    "    # 8 Compare Probabilities\n",
    "    with torch.no_grad():\n",
    "        modified_outputs = model.model(\n",
    "            inputs_embeds=modified_embeddings,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "        original_probs = torch.softmax(outputs.logits[0, target_position], dim=-1)\n",
    "        modified_probs = torch.softmax(modified_outputs.logits[0, target_position], dim=-1)\n",
    "\n",
    "    print(f\"Original probability of '{target_token}': {original_probs[target_id].item():.10f}\")\n",
    "    print(f\"Modified probability of '{target_token}': {modified_probs[target_id].item():.10f}\")\n",
    "\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc76b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified text:  sealed red woman the very \n",
      "Original probability of 'pretty': 0.0000000352\n",
      "Modified probability of 'pretty': 0.0000055360\n",
      "Modified text:  PROC Red insert in at \n",
      "Original probability of 'pretty': 0.0000001047\n",
      "Modified probability of 'pretty': 0.0000033635\n",
      "Modified text:  provisionsSU provision be at en\n",
      "Original probability of 'pretty': 0.0000081268\n",
      "Modified probability of 'pretty': 0.0000543449\n",
      "Modified text:  accordance required provided or plan El\n",
      "Original probability of 'pretty': 0.0000007280\n",
      "Modified probability of 'pretty': 0.0000002629\n",
      "Modified text:  Copyright tuition offer a designs Los\n",
      "Original probability of 'pretty': 0.0000000032\n",
      "Modified probability of 'pretty': 0.0000027397\n",
      "Modified text:  \"/ McGill assign the layout Sw\n",
      "Original probability of 'pretty': 0.0000000013\n",
      "Modified probability of 'pretty': 0.0000031293\n",
      "Modified text:  \"/ randomSet in Settings detox\n",
      "Original probability of 'pretty': 0.0000000828\n",
      "Modified probability of 'pretty': 0.0000086581\n",
      "Modified text: \":{\"Scale76561 and settings unmist\n",
      "Original probability of 'pretty': 0.0000009289\n",
      "Modified probability of 'pretty': 0.0000392307\n",
      "Modified text:  sep srfAttach Psych show Marg seemingly\n",
      "Original probability of 'pretty': 0.0000018130\n",
      "Modified probability of 'pretty': 0.0000512806\n",
      "Modified text:  soul Marketableun Send trend appears\n",
      "Original probability of 'pretty': 0.0000003839\n",
      "Modified probability of 'pretty': 0.0000156626\n",
      "Modified text:  Eye MonsterOrderable Brid skepticism seemed\n",
      "Original probability of 'pretty': 0.0000000001\n",
      "Modified probability of 'pretty': 0.0000055722\n",
      "Modified text:  arcane bounded Female -------- Savage inexpl\n",
      "Original probability of 'pretty': 0.0000000336\n",
      "Modified probability of 'pretty': 0.0000004832\n",
      "Modified text:  insanely insane lemon dep Monk dialect\n",
      "Original probability of 'pretty': 0.0000000412\n",
      "Modified probability of 'pretty': 0.0000254627\n",
      "Modified text:  inhab consciousness utopian Leslie Billy anarch\n",
      "Original probability of 'pretty': 0.0000002055\n",
      "Modified probability of 'pretty': 0.0000047996\n",
      "Modified text:  XY weird comedy Carrie Parks chrom\n",
      "Original probability of 'pretty': 0.0000002693\n",
      "Modified probability of 'pretty': 0.0000406898\n",
      "Modified text:  crystall weird rhyth Hot Pineocaly\n",
      "Original probability of 'pretty': 0.0000001488\n",
      "Modified probability of 'pretty': 0.0000539120\n",
      "Modified text:  Pri ethn Narc hip South synt\n",
      "Original probability of 'pretty': 0.0000000132\n",
      "Modified probability of 'pretty': 0.0001159291\n",
      "Modified text: flake Tumblr cowboy Dirtyhop idiosyncr\n",
      "Original probability of 'pretty': 0.0000032885\n",
      "Modified probability of 'pretty': 0.0000213670\n",
      "Modified text: Material Staticsync sexyRadio Sym\n",
      "Original probability of 'pretty': 0.0000000760\n",
      "Modified probability of 'pretty': 0.0000005210\n",
      "Modified text:  newsletterOrderable AzerbánAIDS singles musical\n",
      "Original probability of 'pretty': 0.0000002526\n",
      "Modified probability of 'pretty': 0.0000144641\n",
      "Modified text:  banterforum southwesternán ARE nu musical\n",
      "Original probability of 'pretty': 0.0000000525\n",
      "Modified probability of 'pretty': 0.0000037765\n",
      "Modified text:  eclectic Sit Irish Native Upper Cr hilar\n",
      "Original probability of 'pretty': 0.0000045487\n",
      "Modified probability of 'pretty': 0.0000080963\n",
      "Modified text:  comedian punk rock indie high Cr syll\n",
      "Original probability of 'pretty': 0.0000000172\n",
      "Modified probability of 'pretty': 0.0000017676\n",
      "Modified text:  curriculum liberalism rock Alvin University Cal Philos\n",
      "Original probability of 'pretty': 0.0000012606\n",
      "Modified probability of 'pretty': 0.0000001969\n",
      "Modified text:  mascot bigotry MLB Mississippi Bay freshmen ASP\n",
      "Original probability of 'pretty': 0.0000000686\n",
      "Modified probability of 'pretty': 0.0000061193\n",
      "Modified text:  Hilton Slate sq Moore sailed campuses magical\n",
      "Original probability of 'pretty': 0.0000002861\n",
      "Modified probability of 'pretty': 0.0000262124\n",
      "Modified text:  Lair Newcastle adul Babylon islands beach Clever\n",
      "Original probability of 'pretty': 0.0000000373\n",
      "Modified probability of 'pretty': 0.0000014396\n",
      "Modified text:  condos Nova Toxic Buffalo Honolulu USPS hipp\n",
      "Original probability of 'pretty': 0.0000006908\n",
      "Modified probability of 'pretty': 0.0000244711\n",
      "Modified text:  RTX cricket continual BuffaloMiami ske Hipp\n",
      "Original probability of 'pretty': 0.0000024381\n",
      "Modified probability of 'pretty': 0.0000521124\n",
      "Modified text:  Vulkan43 triple RTX 164 Express grou\n",
      "Original probability of 'pretty': 0.0000001975\n",
      "Modified probability of 'pretty': 0.0000009621\n",
      "Modified text: Stretch Schl Rays999 conquest Trinity Qu\n",
      "Original probability of 'pretty': 0.0000000000\n",
      "Modified probability of 'pretty': 0.0000000890\n",
      "Modified text: Stretch Shel sparkling012 ignorance Princeton Qu\n",
      "Original probability of 'pretty': 0.0000001091\n",
      "Modified probability of 'pretty': 0.0000209925\n",
      "Modified text:  Athena Sony anomal713 slap Duke Qu\n",
      "Original probability of 'pretty': 0.0000000714\n",
      "Modified probability of 'pretty': 0.0000033886\n",
      "Modified text:  Pipeline Microsoft antib\u0004 violations Princeton Qu\n",
      "Original probability of 'pretty': 0.0000003484\n",
      "Modified probability of 'pretty': 0.0000004555\n",
      "Modified text:  pipeline Cooke antidepressOM violates NY Quantum\n",
      "Original probability of 'pretty': 0.0000000634\n",
      "Modified probability of 'pretty': 0.0000259116\n",
      "Modified text:  OpenGL Courtney TYOS js divorce qu\n",
      "Original probability of 'pretty': 0.0000000657\n",
      "Modified probability of 'pretty': 0.0000187479\n",
      "Modified text:  virginity zoom ClippersOS college sexuality qu\n",
      "Original probability of 'pretty': 0.0000001238\n",
      "Modified probability of 'pretty': 0.0000036697\n",
      "Modified text: /groupon actionGroupOS cunt nig qu\n",
      "Original probability of 'pretty': 0.0000000417\n",
      "Modified probability of 'pretty': 0.0000002105\n",
      "Modified text:  her Gay dramas inverse sexual Cricket Tab\n",
      "Original probability of 'pretty': 0.0000008201\n",
      "Modified probability of 'pretty': 0.0000006752\n",
      "Modified text:  her Claim fucked pseudo Sex Hannah Savage\n",
      "Original probability of 'pretty': 0.0000000548\n",
      "Modified probability of 'pretty': 0.0000167365\n",
      "Modified text:  my divided doom sigh climax Lucius Mongo\n",
      "Original probability of 'pretty': 0.0000004125\n",
      "Modified probability of 'pretty': 0.0000053617\n",
      "Modified text: | unlocked apocalypse pauses Starr weird semi\n",
      "Original probability of 'pretty': 0.0000000860\n",
      "Modified probability of 'pretty': 0.0001278935\n",
      "Modified text: iop deprive Hilbert Jeanne Hilbert rec Po\n",
      "Original probability of 'pretty': 0.0000000098\n",
      "Modified probability of 'pretty': 0.0000000427\n",
      "Modified text:  fung curtail Tracy Bill ShannonPercentlo\n",
      "Original probability of 'pretty': 0.0000000000\n",
      "Modified probability of 'pretty': 0.0001048582\n",
      "Modified text:  blockbuster thwart LeBron Michael Belichick toilet Lo\n",
      "Original probability of 'pretty': 0.0000001615\n",
      "Modified probability of 'pretty': 0.0000013604\n",
      "Modified text:  Recipes prevent Alb Michelle Sick rec Des\n",
      "Original probability of 'pretty': 0.0000001671\n",
      "Modified probability of 'pretty': 0.0001179577\n",
      "Modified text:  cont preventive BlackJane herpes rec Dos\n",
      "Original probability of 'pretty': 0.0000050080\n",
      "Modified probability of 'pretty': 0.0000452015\n",
      "Modified text:  contcancer CrowPie cum 15 Dis\n",
      "Original probability of 'pretty': 0.0000000017\n",
      "Modified probability of 'pretty': 0.0025492311\n",
      "Modified text:  Jets Goo Riley Skin thumbnail Barker Schwarz\n",
      "Original probability of 'pretty': 0.0000175675\n",
      "Modified probability of 'pretty': 0.0000399654\n",
      "Modified text:  glory Leslie javascript indec gun Bow jet\n",
      "Original probability of 'pretty': 0.0000000003\n",
      "Modified probability of 'pretty': 0.0000385700\n",
      "Modified text:  Blitz Bowling curl spicy Cars Quinn airplane\n",
      "Original probability of 'pretty': 0.0000012463\n",
      "Modified probability of 'pretty': 0.0001861184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 50\n",
    "iter_ = 0\n",
    "\n",
    "input_text = \"that white woman is very \"\n",
    "target_token = \"pretty\"  # Token we want to make more favorable\n",
    "\n",
    "new_input = input_text\n",
    "\n",
    "while iter_ <= n : \n",
    "    new_input = optimize_toward_target(input_text=new_input, target_token=target_token)\n",
    "    if new_input == None :\n",
    "        break\n",
    "\n",
    "    iter_+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3165e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dba08116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The quick brown fox jumps over the'\n",
      "Model's predicted next token: ' fence'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 2. Input text\n",
    "input_text = \"The quick brown fox jumps over the\"\n",
    "\n",
    "# 3. Tokenize and get model prediction\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()  # Get last token prediction\n",
    "\n",
    "# 4. Decode the predicted token\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "print(f\"Input: '{input_text}'\")\n",
    "print(f\"Model's predicted next token: '{predicted_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68cab1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Target token ' barking' not found in vocabulary. Using UNK ID (50256). Results might not be meaningful.\n",
      "Modified text: The quick brown fox jumps over the\n",
      "Original probability of ' barking': 0.0000003711\n",
      "Modified probability of ' barking': 0.0004909183\n",
      "\n",
      "Token changes: \n",
      "No change : The\n",
      "No change : Ġquick\n",
      "No change : Ġbrown\n",
      "No change : Ġfox\n",
      "No change : Ġjumps\n",
      "No change : Ġover\n",
      "No change : Ġthe\n"
     ]
    }
   ],
   "source": [
    "from inseq import load_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 1. Load model with gradient-based attribution\n",
    "model = load_model(\"gpt2\", \"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = model.model.get_input_embeddings()\n",
    "\n",
    "# 2. Define input and target output token\n",
    "input_text = \"The quick brown fox jumps over the\"\n",
    "target_token = \" barking\"  # Token we want to make more favorable\n",
    "\n",
    "# 3. Get token IDs\n",
    "inputs = model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "target_id = model.tokenizer.convert_tokens_to_ids(target_token)\n",
    "\n",
    "# # For optimizing toward a phrase:\n",
    "# target_ids = [model.tokenizer.convert_tokens_to_ids(t) for t in [\"lazy\", \"dog\"]]\n",
    "# target_logits = sum(outputs.logits[0, -len(target_ids):, target_ids].diag())\n",
    "\n",
    "if target_id == model.tokenizer.unk_token_id:\n",
    "     print(f\"Warning: Target token '{target_token}' not found in vocabulary. Using UNK ID ({target_id}). Results might not be meaningful.\")\n",
    "\n",
    "# 4. Perform forward pass with gradient tracking\n",
    "with torch.set_grad_enabled(True):\n",
    "    # Get embeddings and mark as requiring gradients\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    embeddings.retain_grad()\n",
    "    embeddings.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass through model\n",
    "    outputs = model.model(\n",
    "        inputs_embeds=embeddings,\n",
    "        attention_mask=inputs.attention_mask\n",
    "    )\n",
    "    \n",
    "    # Get logits for the target position (next token prediction)\n",
    "    target_position = -1  # Position after last input token\n",
    "    target_logit = outputs.logits[0, target_position, target_id]\n",
    "    \n",
    "    # Compute gradients\n",
    "    target_logit.backward()\n",
    "    gradients = embeddings.grad.clone()\n",
    "\n",
    "# # 5. Modify embeddings with gradients\n",
    "modified_embeddings = embeddings + gradients\n",
    "\n",
    "all_embeddings = embedding_layer.weight #Vocab Embeddings\n",
    "new_token_ids = []\n",
    "for i, emb in enumerate(modified_embeddings[0]) : \n",
    "    distances = F.cosine_similarity(emb, all_embeddings, dim=-1)\n",
    "    new_id = torch.argmax(distances).item()\n",
    "    new_token_ids.append(new_id)\n",
    "\n",
    "#Decode the new tokens\n",
    "new_tokens = model.tokenizer.convert_ids_to_tokens(new_token_ids)\n",
    "new_text = model.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "print(f\"Modified text: {new_text}\")\n",
    "\n",
    "# 8 Compare Probabilities\n",
    "with torch.no_grad():\n",
    "    modified_outputs = model.model(\n",
    "        inputs_embeds=modified_embeddings,\n",
    "        attention_mask=inputs.attention_mask\n",
    "    )\n",
    "    original_probs = torch.softmax(outputs.logits[0, target_position], dim=-1)\n",
    "    modified_probs = torch.softmax(modified_outputs.logits[0, target_position], dim=-1)\n",
    "\n",
    "print(f\"Original probability of '{target_token}': {original_probs[target_id].item():.10f}\")\n",
    "print(f\"Modified probability of '{target_token}': {modified_probs[target_id].item():.10f}\")\n",
    "\n",
    "\n",
    "# Show token substitutions\n",
    "print(\"\\nToken changes: \")\n",
    "for orig, new in zip(model.tokenizer.tokenize(input_text), new_tokens):\n",
    "    if orig != new: \n",
    "        print(f\"{orig} -> {new}\")\n",
    "    else : \n",
    "        print(\"No change :\", orig)\n",
    "# # Optional: Visualize gradient magnitudes\n",
    "# gradient_magnitudes = torch.norm(gradients, dim=-1).squeeze()\n",
    "# print(\"\\nGradient magnitudes per input token:\")\n",
    "# for token, magnitude in zip(model.tokenizer.convert_ids_to_tokens(inputs.input_ids[0]), gradient_magnitudes):\n",
    "#     print(f\"{token:>10}: {magnitude.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43bad54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db1ea9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad1ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8473b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de53020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b37fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b429d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1dcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf465fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\XAI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import inseq\n",
    "from inseq import load_model\n",
    "# from inseq.models import InseqModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae382fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'\n",
    "INPUT_TEXT = \"The quick fox jumps over the lazy \"\n",
    "TARGET_OUTPUT_TOKEN = \"dog\"\n",
    "TARGET_OUTPUT_POSITION = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d43c10b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\XAI\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"gpt2\", \"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b6338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token = model.tokenizer(INPUT_TEXT, return_tensors='pt')\n",
    "input_ids = input_token[\"input_ids\"].to(model.device)\n",
    "attention_mask = input_token[\"attention_mask\"].to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19e04383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token 'dog' corresponds to ID: 9703\n"
     ]
    }
   ],
   "source": [
    "target_token_id = model.tokenizer.convert_tokens_to_ids(TARGET_OUTPUT_TOKEN)\n",
    "if target_token_id == model.tokenizer.unk_token_id: \n",
    "    print(f\"Warning: Target token '{TARGET_OUTPUT_TOKEN}' not found in vocabulary. Using UNK ID ({target_token_id}). Results might not be meaningful.\")\n",
    "\n",
    "print(f\"Target token '{TARGET_OUTPUT_TOKEN}' corresponds to ID: {target_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62882d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e051c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input embeddings\n",
    "# with torch.set_grad_enabled(True) : \n",
    "original_embeddings = model.model.get_input_embeddings()(input_ids)\n",
    "original_embeddings.clone().detach().requires_grad_(True)\n",
    "\n",
    "if hasattr(model.model, 'encoder') and hasattr(model.model, 'decoder'): \n",
    "    print(\"Detected Seq2Seq model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
