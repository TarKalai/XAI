{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad64bf43",
   "metadata": {},
   "source": [
    "### General Optimisation of Input Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "85d69dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inseq import load_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Load model with gradient-based attribution\n",
    "model = load_model(model=\"gpt2\", attribution_method=\"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = model.model.get_input_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f1da859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_toward_target(input_text, target_token, step) : \n",
    "    \"\"\"Change the input tokens in such way that the probability of getting the target_token is increased.\n",
    "    The optimization is done for all input tokens at once (in parallel)\n",
    "\n",
    "    Args:\n",
    "        input_text (_type_): sequence of input text (str)\n",
    "        target_token (_type_): the next token that amy be given by the model to compete the input text (str)\n",
    "        step (_type_): factor by which the gradient is multiplied to move in the embedding space (int)\n",
    "\n",
    "    Returns:\n",
    "        _type_: new sequence of input text (str)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert input text to token \n",
    "    inputs = model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "    target_token = model.tokenizer.tokenize(\" \" + target_token.strip()) # make sur the target output word starts with a space to ahve a recognised token\n",
    "    if len(target_token) == 1: \n",
    "        target_token = target_token[0]\n",
    "    else: \n",
    "        print(f\"Several tokens have been assigned to the target output word: {target_token}.\")\n",
    "        return None\n",
    "    target_id = model.tokenizer.convert_tokens_to_ids(target_token) # convert the target token to ID\n",
    "\n",
    "    # # For optimizing toward a phrase:\n",
    "    # target_ids = [model.tokenizer.convert_tokens_to_ids(t) for t in [\"lazy\", \"dog\"]]\n",
    "    # target_logits = sum(outputs.logits[0, -len(target_ids):, target_ids].diag())\n",
    "\n",
    "    # Stop if target token is not part of the vocabulary of gpt2\n",
    "    if target_id == model.tokenizer.unk_token_id:\n",
    "        print(f\"Warning: Target token '{target_token}' not found in vocabulary. Using UNK ID ({target_id}). Results might not be meaningful.\")\n",
    "        return None\n",
    "\n",
    "    # Perform forward pass with gradient tracking\n",
    "    with torch.set_grad_enabled(True):\n",
    "        # Get embeddings and mark as requiring gradients (so that we retain the gradient for non-leaf tensors)\n",
    "        embeddings = embedding_layer(inputs.input_ids)\n",
    "        embeddings.retain_grad()\n",
    "        embeddings.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass through model\n",
    "        outputs = model.model(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get logits for the target position (next token prediction)\n",
    "        target_position = -1  # Position after last input token\n",
    "        target_logit = outputs.logits[0, target_position, target_id]\n",
    "        \n",
    "        # Compute gradients d(logit)/d(embeddings)\n",
    "        target_logit.backward()\n",
    "        gradients = embeddings.grad.clone()\n",
    "\n",
    "    # Modify embeddings with gradients\n",
    "    # print(\"Comuted Gradients: \", gradients)\n",
    "    for i in range(len(gradients[0])) : \n",
    "        gradients[0][i] = gradients[0][i] / (sum(abs(i) for i in gradients[0][i])) # compute 1 step gradient (divide by norm the gradient)\n",
    "\n",
    "    modified_embeddings = embeddings + gradients*step\n",
    "\n",
    "    # Nearest Neighbor Search\n",
    "    all_embeddings = embedding_layer.weight #Vocab Embeddings; all tokens in vocabulary\n",
    "    new_token_ids = []\n",
    "    for i, emb in enumerate(modified_embeddings[0]) : \n",
    "        # find closest token in voc. Best to use cos similarity as it will ignore the magnitude of the embeddings and focus on the direction\n",
    "        # cos(a, b) = (a . b) / (||a|| * ||b||)\n",
    "        distances = F.cosine_similarity(emb, all_embeddings, dim=-1)\n",
    "\n",
    "        # # Euclidean distance \n",
    "        # distances = -torch.norm(all_embeddings - emb, dim=1)\n",
    "        # # Dot product (ignore magnitude normalization here)\n",
    "        # distances = torch.matmul(all_embeddings, emb)\n",
    "\n",
    "        new_id = torch.argmax(distances).item()\n",
    "        new_token_ids.append(new_id)\n",
    "\n",
    "    #Decode the new tokens\n",
    "    new_tokens = model.tokenizer.convert_ids_to_tokens(new_token_ids)\n",
    "    new_text = model.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "    print(f\"Original text: {input_text}\")\n",
    "    print(f\"Modified text: {new_text}\")\n",
    "\n",
    "\n",
    "    # Compare Probabilities\n",
    "    with torch.no_grad():\n",
    "        modified_outputs = model.model(\n",
    "            inputs_embeds=modified_embeddings,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "        original_probs = torch.softmax(outputs.logits[0, target_position], dim=-1)\n",
    "        modified_probs = torch.softmax(modified_outputs.logits[0, target_position], dim=-1)\n",
    "\n",
    "    print(f\"Original probability of '{target_token}': {original_probs[target_id].item():.10f}\")\n",
    "    print(f\"Modified probability of '{target_token}': {modified_probs[target_id].item():.10f}\")\n",
    "\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4f7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d78cf4",
   "metadata": {},
   "source": [
    "### Specific Input Token Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inseq import load_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Load model with gradient-based attribution\n",
    "model = load_model(model=\"gpt2\", attribution_method=\"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = model.model.get_input_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a294a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_single_input_token_toward_target(input_text, target_token, step, n_iteration, token_input_position) : \n",
    "\n",
    "    target_token = model.tokenizer.tokenize(\" \" + target_token.strip()) # make sur the target output word starts with a space to ahve a recognised token\n",
    "    if len(target_token) == 1: \n",
    "        target_token = target_token[0]\n",
    "    else: \n",
    "        print(f\"Several tokens have been assigned to the target output word: {target_token}.\")\n",
    "        return None\n",
    "    target_id = model.tokenizer.convert_tokens_to_ids(target_token) # convert the target token to ID\n",
    "\n",
    "    if target_id == model.tokenizer.unk_token_id:\n",
    "        print(f\"Warning: Target token '{target_token}' not found in vocabulary. Using UNK ID ({target_id}). Results might not be meaningful.\")\n",
    "        return None\n",
    "    \n",
    "    original_text = input_text\n",
    "    for i in range(n_iteration) :\n",
    "        \n",
    "        inputs = model.tokenizer(original_text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Get embeddings and mark as requiring gradients (so that we retain the gradient for non-leaf tensors)\n",
    "            embeddings = embedding_layer(inputs.input_ids)\n",
    "            embeddings.retain_grad()\n",
    "            embeddings.requires_grad_(True)\n",
    "            \n",
    "            # Forward pass through model\n",
    "            outputs = model.model(\n",
    "                inputs_embeds=embeddings,\n",
    "                attention_mask=inputs.attention_mask\n",
    "            )\n",
    "            # Get logits for the target position (next token prediction)\n",
    "            target_position = -1  # Position after last input token\n",
    "            target_logit = outputs.logits[0, target_position, target_id]\n",
    "            \n",
    "            # Compute gradients d(logit)/d(embeddings)\n",
    "            target_logit.backward()\n",
    "            gradients = embeddings.grad.clone()\n",
    "\n",
    "        # Modify embeddings with gradients\n",
    "        # print(\"Comuted Gradients: \", gradients)\n",
    "        for i in range(len(gradients[0])) : \n",
    "            gradients[0][i] = gradients[0][i] / (sum(abs(i) for i in gradients[0][i])) # compute 1 step gradient (divide by norm the gradient)\n",
    "\n",
    "        modified_embeddings = embeddings\n",
    "\n",
    "        modified_embeddings[0][token_input_position] += gradients[0][token_input_position]*step # only for specific input token\n",
    "        # + 0.9*(gradients[0].mean(dim=0)) #consider the context (0.5*mean of the input embeddings)\n",
    "\n",
    "        # Nearest Neighbor Search\n",
    "        all_embeddings = embedding_layer.weight #Vocab Embeddings; all tokens in vocabulary\n",
    "        new_token_ids = []\n",
    "        distances = F.cosine_similarity(modified_embeddings[0][token_input_position], all_embeddings, dim=-1)\n",
    "        new_id = torch.argmax(distances).item()\n",
    "        new_token_ids.append(new_id)\n",
    "        inputs['input_ids'][0][token_input_position] = new_token_ids[0]\n",
    "        new_token_ids = inputs['input_ids']\n",
    "\n",
    "        # print(\"DEBUG: \", new_token_ids)\n",
    "\n",
    "\n",
    "        #Decode the new tokens\n",
    "        new_tokens = model.tokenizer.convert_ids_to_tokens(new_token_ids[0])\n",
    "        new_text = model.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "        print(f\"Original text: {original_text}\")\n",
    "        print(f\"Modified text: {new_text}\")\n",
    "\n",
    "\n",
    "        # Compare Probabilities\n",
    "        with torch.no_grad():\n",
    "            modified_outputs = model.model(\n",
    "                inputs_embeds=modified_embeddings,\n",
    "                attention_mask=inputs.attention_mask\n",
    "            )\n",
    "            original_probs = torch.softmax(outputs.logits[0, target_position], dim=-1)\n",
    "            modified_probs = torch.softmax(modified_outputs.logits[0, target_position], dim=-1)\n",
    "\n",
    "        print(f\"Original probability of '{target_token}': {original_probs[target_id].item():.10f}\")\n",
    "        print(f\"Modified probability of '{target_token}': {modified_probs[target_id].item():.10f}\")\n",
    "\n",
    "        original_text = new_text\n",
    "\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "463322d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: that black woman is\n",
      "Modified text: that red woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0020418002\n",
      "Original text: that red woman is\n",
      "Modified text: that thus woman is\n",
      "Original probability of 'Ġpretty': 0.0031131522\n",
      "Modified probability of 'Ġpretty': 0.0003907612\n",
      "Original text: that thus woman is\n",
      "Modified text: that thus woman is\n",
      "Original probability of 'Ġpretty': 0.0001825135\n",
      "Modified probability of 'Ġpretty': 0.0016098868\n",
      "Original text: that thus woman is\n",
      "Modified text: that thus woman is\n",
      "Original probability of 'Ġpretty': 0.0001825135\n",
      "Modified probability of 'Ġpretty': 0.0016098868\n",
      "Original text: that thus woman is\n",
      "Modified text: that thus woman is\n",
      "Original probability of 'Ġpretty': 0.0001825135\n",
      "Modified probability of 'Ġpretty': 0.0016098868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'that thus woman is'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"that black woman is\"\n",
    "target_token = \"pretty\"\n",
    "optimize_single_input_token_toward_target(input_text=input_text, target_token=target_token, step=512, token_input_position=1, n_iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6b1f9",
   "metadata": {},
   "source": [
    "### Integrated Gradients Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cdfdee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inseq import load_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Load model with gradient-based attribution\n",
    "model = load_model(model=\"gpt2\", attribution_method=\"integrated_gradients\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = model.model.get_input_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "377bbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_specific_input_token_with_IG_towards_target_1(input_text, target_token, step, n_iteration, input_token_pos) : \n",
    "\n",
    "    target_token = model.tokenizer.tokenize(\" \" + target_token.strip()) # make sur the target output word starts with a space to ahve a recognised token\n",
    "    if len(target_token) == 1: \n",
    "        target_token = target_token[0]\n",
    "    else: \n",
    "        print(f\"Several tokens have been assigned to the target output word: {target_token}.\")\n",
    "        return None\n",
    "    target_id = model.tokenizer.convert_tokens_to_ids(target_token) # convert the target token to ID\n",
    "\n",
    "    if target_id == model.tokenizer.unk_token_id:\n",
    "        print(f\"Warning: Target token '{target_token}' not found in vocabulary. Using UNK ID ({target_id}). Results might not be meaningful.\")\n",
    "        return None\n",
    "    \n",
    "    original_text = input_text\n",
    "    for i in range(n_iteration) :\n",
    "        \n",
    "        inputs = model.tokenizer(original_text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Get embeddings and mark as requiring gradients (so that we retain the gradient for non-leaf tensors)\n",
    "            embeddings = embedding_layer(inputs.input_ids)\n",
    "            embeddings.retain_grad()\n",
    "            embeddings.requires_grad_(True)\n",
    "            \n",
    "            # Forward pass through model\n",
    "            outputs = model.model(\n",
    "                inputs_embeds=embeddings,\n",
    "                attention_mask=inputs.attention_mask\n",
    "            )\n",
    "            # Get logits for the target position (next token prediction)\n",
    "            target_position = -1  # Position after last input token\n",
    "            target_logit = outputs.logits[0, target_position, target_id]\n",
    "            \n",
    "            # Compute gradients d(logit)/d(embeddings)\n",
    "            target_logit.backward()\n",
    "            gradients = embeddings.grad.clone()\n",
    "\n",
    "        # Modify embeddings with gradients\n",
    "        # print(\"Comuted Gradients: \", gradients)\n",
    "        for i in range(len(gradients[0])) : \n",
    "            gradients[0][i] = gradients[0][i] / (sum(abs(i) for i in gradients[0][i])) # compute 1 step gradient (divide by norm the gradient)\n",
    "\n",
    "        modified_embeddings = embeddings\n",
    "\n",
    "        modified_embeddings[0][input_token_pos] += gradients[0][input_token_pos]*step # only for specific input token\n",
    "        # + 0.9*(gradients[0].mean(dim=0)) #consider the context (0.5*mean of the input embeddings)\n",
    "\n",
    "        # Nearest Neighbor Search\n",
    "        all_embeddings = embedding_layer.weight #Vocab Embeddings; all tokens in vocabulary\n",
    "        new_token_ids = []\n",
    "        distances = F.cosine_similarity(modified_embeddings[0][input_token_pos], all_embeddings, dim=-1)\n",
    "        new_id = torch.argmax(distances).item()\n",
    "        new_token_ids.append(new_id)\n",
    "        inputs['input_ids'][0][input_token_pos] = new_token_ids[0]\n",
    "        new_token_ids = inputs['input_ids']\n",
    "\n",
    "        # print(\"DEBUG: \", new_token_ids)\n",
    "\n",
    "\n",
    "        #Decode the new tokens\n",
    "        new_tokens = model.tokenizer.convert_ids_to_tokens(new_token_ids[0])\n",
    "        new_text = model.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "        print(f\"Original text: {original_text}\")\n",
    "        print(f\"Modified text: {new_text}\")\n",
    "\n",
    "\n",
    "        # Compare Probabilities\n",
    "        with torch.no_grad():\n",
    "            modified_outputs = model.model(\n",
    "                inputs_embeds=modified_embeddings,\n",
    "                attention_mask=inputs.attention_mask\n",
    "            )\n",
    "            original_probs = torch.softmax(outputs.logits[0, target_position], dim=-1)\n",
    "            modified_probs = torch.softmax(modified_outputs.logits[0, target_position], dim=-1)\n",
    "\n",
    "        print(f\"Original probability of '{target_token}': {original_probs[target_id].item():.10f}\")\n",
    "        print(f\"Modified probability of '{target_token}': {modified_probs[target_id].item():.10f}\")\n",
    "\n",
    "        original_text = new_text\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71710743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_specific_input_token_with_IG_towards_target_2(input_text, target_token, input_token_pos, n_iteration, step): \n",
    "    \n",
    "    target_token = \" \" + target_token.strip()\n",
    "    target_token =  model.tokenizer.tokenize(target_token)[0]\n",
    "    target_id = model.tokenizer.convert_tokens_to_ids(target_token[0])\n",
    "\n",
    "    current_text = input_text\n",
    "    for i in range(n_iteration) :  \n",
    "        original_text = current_text\n",
    "        output = model.attribute(\n",
    "            current_text, \n",
    "            f\"{current_text}{target_token[0]}\", \n",
    "            attributed_fn=\"probability\", \n",
    "            step_scores=[\"probability\"],\n",
    "            show_progress=False\n",
    "        )\n",
    "\n",
    "\n",
    "        grad =  output.sequence_attributions[0].target_attributions[input_token_pos]\n",
    "        grad = grad /  (torch.norm(grad, p=1))\n",
    "\n",
    "        inputs = model.tokenizer(current_text, return_tensors='pt')\n",
    "        embeddings = model.model.get_input_embeddings()(inputs.input_ids)\n",
    "        modified_embeddings = embeddings[0, input_token_pos] + step*grad\n",
    "\n",
    "        all_embeddings = model.model.get_input_embeddings().weight\n",
    "        \n",
    "        distances  = F.cosine_similarity(modified_embeddings[0, input_token_pos], \n",
    "                                         all_embeddings,  \n",
    "                                         dim=-1)\n",
    "        new_id = torch.argmax(distances).item()\n",
    "        new_token = model.tokenizer.convert_ids_to_tokens([new_id])[0]\n",
    "\n",
    "        current_tokens = model.tokenizer.tokenize(current_text)\n",
    "        print(\"INPUT TOKENS: \", current_tokens)\n",
    "\n",
    "        if (input_token_pos != 0) & (new_token[0] != \"Ġ\"): \n",
    "            new_token = \"Ġ\" + new_token\n",
    "\n",
    "        current_tokens[input_token_pos] = new_token\n",
    "        current_text = model.tokenizer.convert_tokens_to_string(current_tokens)\n",
    "        print(\"DEBUG: \", new_token)\n",
    "\n",
    "\n",
    "        orig_prob = output.sequence_attributions[0].step_scores['probability'][0].item()\n",
    "        new_output = model.attribute(\n",
    "            current_text,\n",
    "            f\"{current_text}{target_token[0]}\",\n",
    "            attributed_fn=\"probability\",\n",
    "            step_scores=[\"probability\"],\n",
    "            show_progress=False, \n",
    "        )\n",
    "        new_prob = new_output.sequence_attributions[0].step_scores[\"probability\"][0].item()\n",
    "        \n",
    "        print(f\"Modified: {model.tokenizer.tokenize(original_text)[input_token_pos]} → {new_token}\")\n",
    "        print(f\"Full text: {current_text}\")\n",
    "        print(f\"P({target_token}): {orig_prob:.10f} → {new_prob:.10f} ({'↑' if new_prob > orig_prob else '↓'})\")\n",
    "\n",
    "    return current_text\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1e5cdd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TOKENS:  ['that', 'Ġblack', 'Ġwoman', 'Ġis']\n",
      "DEBUG:  Ġ://\n",
      "Modified: Ġwoman → Ġ://\n",
      "Full text: that black :// is\n",
      "P(Ġpretty): 0.0000000184 → 0.0000009801 (↑)\n",
      "INPUT TOKENS:  ['that', 'Ġblack', 'Ġ:', '//', 'Ġis']\n",
      "DEBUG:  Ġelsius\n",
      "Modified: Ġ: → Ġelsius\n",
      "Full text: that black elsius// is\n",
      "P(Ġpretty): 0.0000009801 → 0.0000007678 (↓)\n",
      "INPUT TOKENS:  ['that', 'Ġblack', 'Ġel', 's', 'ius', '//', 'Ġis']\n",
      "DEBUG:  Ġ://\n",
      "Modified: Ġel → Ġ://\n",
      "Full text: that black ://sius// is\n",
      "P(Ġpretty): 0.0000007678 → 0.0000007765 (↑)\n",
      "INPUT TOKENS:  ['that', 'Ġblack', 'Ġ:', '//', 's', 'ius', '//', 'Ġis']\n",
      "DEBUG:  Ġelsius\n",
      "Modified: Ġ: → Ġelsius\n",
      "Full text: that black elsius//sius// is\n",
      "P(Ġpretty): 0.0000007765 → 0.0000011018 (↑)\n",
      "INPUT TOKENS:  ['that', 'Ġblack', 'Ġel', 's', 'ius', '//', 's', 'ius', '//', 'Ġis']\n",
      "DEBUG:  Ġ://\n",
      "Modified: Ġel → Ġ://\n",
      "Full text: that black ://sius//sius// is\n",
      "P(Ġpretty): 0.0000011018 → 0.0000013532 (↑)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'that black ://sius//sius// is'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"that black woman is\"\n",
    "target_token = \"pretty\" \n",
    "\n",
    "optimize_specific_input_token_with_IG_towards_target_2(input_text=input_text, target_token=target_token, input_token_pos=2, n_iteration=5, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02303b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(277.1607)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm(gradients[0][0], ord=1)\n",
    "sum(abs(i) for i in gradients[0][0])\n",
    "gradients[0][0].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc76b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n",
      "Original text: that black woman is\n",
      "Modified text: that black woman is\n",
      "Original probability of 'Ġpretty': 0.0013306340\n",
      "Modified probability of 'Ġpretty': 0.0015035227\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 20\n",
    "iter_ = 0\n",
    "\n",
    "input_text = \"that black woman is\"\n",
    "target_token = \"pretty\"  # Token we want to make more favorable\n",
    "\n",
    "new_input = input_text\n",
    "\n",
    "while iter_ <= n : \n",
    "    new_input = optimize_toward_target(input_text=new_input, target_token=target_token, step = 5)\n",
    "    if new_input == None :\n",
    "        break\n",
    "\n",
    "    iter_+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f3165e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.4591e-03, -2.0714e-04,  7.3177e-04, -1.0140e-03,  8.6816e-04,\n",
       "        -2.0273e-03, -3.3901e-04,  1.4948e-03, -2.7162e-03, -2.9139e-03,\n",
       "        -2.6725e-03,  5.4865e-04,  1.8390e-03,  1.7984e-03, -9.2928e-04,\n",
       "         2.0346e-03,  3.7214e-03, -1.2461e-03, -2.0981e-04,  1.0690e-04,\n",
       "         2.6508e-03, -3.0332e-03,  4.2063e-03, -1.6150e-03, -3.9101e-03,\n",
       "        -3.3079e-04,  3.1795e-04, -1.1389e-03, -1.6902e-03,  4.0789e-03,\n",
       "        -1.9345e-03, -4.4975e-04,  1.9578e-03,  7.3786e-04,  2.8309e-03,\n",
       "         2.8711e-04, -4.3191e-04, -1.1351e-03,  4.2762e-03, -1.5933e-03,\n",
       "         2.3057e-03,  2.2046e-03, -1.5345e-03,  1.3337e-03,  1.6590e-03,\n",
       "        -6.4271e-03,  1.2239e-03, -1.7001e-03,  1.5255e-03, -1.8939e-04,\n",
       "         3.0685e-04,  2.6848e-04, -1.3084e-04,  1.2047e-03, -1.5419e-03,\n",
       "        -1.9634e-04,  1.1819e-03, -1.9331e-03, -1.6991e-03,  4.2700e-03,\n",
       "        -1.0560e-03,  3.0874e-03,  1.4234e-03,  1.8472e-04,  1.5561e-03,\n",
       "         7.2009e-04,  4.2875e-03,  1.2394e-03, -2.7793e-04, -1.7003e-03,\n",
       "         1.9501e-03,  6.9184e-04, -2.5375e-03, -2.4865e-04, -1.1839e-03,\n",
       "        -1.6804e-03, -1.7880e-03, -3.5867e-04,  2.1051e-04,  9.3478e-04,\n",
       "         1.5856e-03, -1.2273e-03,  3.8236e-03, -2.0723e-04, -4.6818e-04,\n",
       "        -2.5423e-04, -9.4742e-05,  1.4787e-03, -7.6987e-04, -3.1721e-03,\n",
       "        -3.2442e-03,  6.7889e-05,  4.1709e-04, -3.1496e-04,  2.1728e-03,\n",
       "        -3.0396e-03, -4.5553e-03,  6.7584e-05, -1.0239e-03, -1.5001e-04,\n",
       "         4.8309e-04,  2.5595e-03,  2.8717e-04,  4.9230e-04,  3.3000e-03,\n",
       "        -2.0754e-03,  1.8907e-03, -3.2514e-04, -1.8852e-03, -4.8334e-04,\n",
       "         1.4942e-03,  2.1782e-03,  1.5894e-03,  1.7353e-04, -3.9415e-03,\n",
       "         1.9158e-03,  1.7328e-04,  7.5586e-04, -2.6429e-03, -5.3688e-04,\n",
       "        -2.5934e-03,  2.2211e-03,  1.8165e-03,  2.0497e-03,  1.2325e-03,\n",
       "        -2.2513e-03, -1.0061e-04, -4.5203e-04, -2.8997e-04, -2.0081e-03,\n",
       "        -2.4861e-03,  4.7501e-04, -3.4755e-04, -8.2380e-04,  1.6401e-04,\n",
       "         4.4629e-03, -1.4865e-03,  1.8302e-03, -1.9380e-03, -1.1740e-04,\n",
       "         2.0344e-03, -1.8711e-03,  4.4159e-04,  1.0257e-03,  6.1302e-04,\n",
       "        -4.5022e-04, -1.8962e-04, -9.2209e-04, -9.5460e-06, -4.0428e-04,\n",
       "        -4.9511e-04, -5.4754e-04,  2.0546e-03, -8.7571e-05, -5.9661e-04,\n",
       "         4.8997e-04, -3.6903e-03, -9.0205e-04,  2.0791e-03, -1.1973e-04,\n",
       "        -4.5702e-04,  1.5853e-05, -3.2046e-03, -1.7875e-03,  3.6810e-04,\n",
       "         1.2249e-03, -1.2537e-03,  1.9632e-04, -3.9105e-05, -1.5438e-03,\n",
       "         8.5236e-05,  2.4938e-03, -1.8726e-04, -1.3410e-03,  1.0737e-04,\n",
       "         5.8206e-04,  1.1576e-03, -5.2704e-03,  5.3113e-04,  4.8482e-04,\n",
       "        -1.3092e-04, -2.4040e-03,  1.4299e-03,  2.2613e-03, -8.5651e-04,\n",
       "        -6.9635e-05, -3.7876e-03, -2.3825e-03,  1.4803e-03,  6.5881e-04,\n",
       "         5.2154e-04,  1.5920e-03,  2.8039e-03,  1.1715e-03, -4.8513e-04,\n",
       "         2.3420e-04,  6.2623e-04,  4.1113e-04,  3.0439e-03,  9.2854e-05,\n",
       "        -9.9762e-04,  2.9223e-03, -2.0320e-03, -1.9916e-04,  1.0861e-03,\n",
       "         2.3423e-03,  2.3315e-03,  2.0721e-03,  5.2108e-04, -1.3381e-03,\n",
       "         2.0827e-03, -5.7141e-04, -1.3303e-03,  1.7911e-04,  1.6143e-03,\n",
       "         9.7331e-04,  8.6334e-04,  5.2728e-04, -7.3249e-04, -1.8628e-03,\n",
       "        -2.2962e-03, -1.6527e-03, -2.2578e-03, -6.9430e-05,  6.9449e-04,\n",
       "        -3.2128e-03, -3.8022e-04, -1.1130e-03, -4.7414e-03,  1.3794e-03,\n",
       "         3.6317e-04,  1.1973e-03,  1.9269e-03,  1.6637e-03, -2.8079e-03,\n",
       "         1.1293e-03,  1.0235e-03,  1.0650e-03,  1.4690e-03, -2.6266e-03,\n",
       "         7.1761e-04, -1.4627e-03,  8.9031e-04, -1.2010e-04,  2.4070e-03,\n",
       "         3.2439e-03,  6.3577e-04,  7.7934e-04, -3.3606e-03,  1.4055e-03,\n",
       "         3.7054e-04, -5.8136e-04, -8.0554e-04,  3.2887e-03,  9.3241e-04,\n",
       "        -9.8783e-04, -9.6663e-04, -1.0057e-03, -7.1783e-04, -1.1231e-03,\n",
       "        -5.8417e-04, -4.9142e-04,  2.4292e-03,  3.6287e-03,  3.0210e-03,\n",
       "        -2.4042e-03,  1.5546e-03, -5.9767e-04,  9.7875e-04,  1.3521e-04,\n",
       "         1.2556e-05, -2.0576e-04, -4.8547e-04,  1.6030e-03, -3.5028e-04,\n",
       "         9.0599e-04,  1.9906e-04, -1.2668e-03, -3.0422e-03,  2.8453e-03,\n",
       "        -2.0109e-03,  8.1508e-04, -4.8745e-04, -3.8550e-03, -8.2678e-04,\n",
       "        -1.8518e-03, -1.5454e-03, -1.4484e-03, -1.7152e-04, -3.3749e-04,\n",
       "        -5.5352e-04,  2.2263e-03, -2.3545e-03,  7.6221e-04, -2.1120e-03,\n",
       "         5.0731e-04, -1.0218e-03,  9.5557e-04,  1.5774e-04,  5.6788e-04,\n",
       "        -3.8795e-05,  3.0401e-03, -8.5880e-04, -1.5689e-03,  1.2576e-03,\n",
       "        -3.5060e-03, -1.1164e-03, -4.1553e-03,  9.6451e-05,  7.2992e-04,\n",
       "         1.6717e-03, -9.6743e-05, -5.2761e-04, -6.5818e-04,  1.1350e-03,\n",
       "         3.5127e-03, -3.7067e-04,  2.2692e-03,  3.5016e-03, -2.9712e-04,\n",
       "         5.0777e-04, -3.0171e-03, -3.2652e-03, -5.1256e-04,  3.8708e-04,\n",
       "        -3.2786e-03,  2.8767e-04, -4.1584e-03, -1.0767e-03,  1.2477e-03,\n",
       "        -1.8395e-03,  2.3335e-03, -7.3519e-04,  4.9063e-04,  1.6751e-03,\n",
       "         2.0576e-03, -1.1594e-03, -7.3137e-03, -2.1106e-03,  2.4048e-03,\n",
       "        -4.2088e-04,  2.2399e-03,  2.5875e-03, -7.7021e-04, -3.0831e-04,\n",
       "        -4.2104e-04, -4.0455e-04, -2.1643e-03,  6.1895e-04,  1.2581e-03,\n",
       "        -2.3697e-03, -4.0381e-05,  3.7975e-04, -6.1485e-04,  8.2237e-04,\n",
       "         2.2619e-03,  1.9853e-05, -6.1707e-05,  1.5755e-03,  1.7459e-03,\n",
       "         3.2415e-03, -1.3366e-04, -3.4253e-04,  2.6495e-03,  1.9372e-03,\n",
       "         5.6852e-04,  1.3023e-03, -1.7616e-03, -1.1845e-03,  1.8161e-03,\n",
       "        -4.8583e-04,  6.1955e-04, -6.3075e-04,  1.7120e-03,  5.6035e-04,\n",
       "         2.3965e-04,  4.6846e-04,  1.2345e-04,  1.5170e-03,  3.7174e-03,\n",
       "         1.8091e-03,  4.4408e-04,  1.7701e-04,  8.6050e-04, -1.6552e-03,\n",
       "         1.8046e-03,  7.6316e-04, -1.1932e-03, -3.0007e-03, -4.5104e-05,\n",
       "        -1.9725e-03,  9.4185e-04,  6.5244e-04, -6.0828e-04,  2.3087e-03,\n",
       "        -9.2486e-04,  7.0341e-04, -3.2899e-03, -1.7345e-03,  5.6279e-03,\n",
       "         1.3102e-03, -1.4055e-03,  2.8009e-05, -2.5184e-03, -3.6986e-03,\n",
       "        -1.8402e-04, -1.6491e-03, -4.1460e-03, -1.2589e-03, -1.1686e-03,\n",
       "        -7.2512e-04,  6.5781e-04, -6.4168e-04,  1.6769e-05,  4.7910e-04,\n",
       "         8.3297e-04,  2.5817e-03,  9.6098e-04, -1.6742e-03,  6.0761e-04,\n",
       "         1.0594e-04,  1.1767e-03,  7.3024e-04, -1.3299e-03, -1.1516e-03,\n",
       "        -1.7505e-03,  1.2744e-03,  1.3267e-03,  6.4600e-04, -5.3855e-04,\n",
       "        -3.9226e-04, -2.5897e-03, -8.2248e-04,  8.8508e-04, -1.8843e-03,\n",
       "         1.7670e-04, -1.1919e-03,  2.4307e-03,  2.2983e-03,  5.3246e-05,\n",
       "         1.0656e-03, -2.4094e-03,  8.4485e-06,  8.5224e-04, -1.7404e-03,\n",
       "        -1.5620e-03,  1.5265e-03, -1.7373e-03,  1.7885e-04,  2.7944e-03,\n",
       "         2.3729e-04, -2.4234e-03, -1.5675e-04, -2.3422e-04,  1.4196e-04,\n",
       "         3.6515e-04, -7.8747e-05,  1.9052e-03,  2.7392e-03, -4.4366e-05,\n",
       "        -2.2590e-03, -5.7247e-04,  1.8483e-03, -3.2666e-03,  7.7346e-04,\n",
       "        -3.0094e-03, -1.9207e-03,  2.5672e-03,  8.7684e-04, -1.7043e-04,\n",
       "         6.3760e-04,  4.6490e-04,  4.5034e-04,  1.1513e-03,  4.0675e-04,\n",
       "        -1.0034e-03,  1.8165e-03,  9.7802e-04,  1.5772e-04, -1.1563e-05,\n",
       "         1.3105e-03, -1.9359e-03,  4.7730e-04,  3.5508e-03, -9.5763e-04,\n",
       "        -2.1703e-03, -5.1650e-04,  6.7586e-04, -1.5964e-03,  2.6390e-03,\n",
       "         6.8016e-04,  3.9465e-03, -1.4085e-03, -3.9341e-04, -7.7267e-05,\n",
       "        -3.1194e-04, -3.7560e-04, -1.6665e-04, -7.9412e-04, -8.7021e-05,\n",
       "        -1.0293e-04,  2.4716e-03,  7.5353e-04,  6.2904e-05,  3.2975e-03,\n",
       "         1.3777e-03,  1.6298e-03,  2.7368e-03,  1.1135e-03, -1.5685e-03,\n",
       "        -5.3381e-03, -1.9422e-04, -5.1940e-04,  3.4635e-04,  1.1177e-03,\n",
       "         2.7929e-03, -9.0757e-04, -3.8610e-04, -1.7246e-03,  7.0399e-04,\n",
       "         1.5788e-03, -5.1257e-04,  3.1485e-03, -8.6275e-04,  1.8435e-04,\n",
       "         2.1256e-04,  8.3504e-04,  5.7569e-04, -1.7823e-04,  3.5081e-04,\n",
       "         1.4972e-03, -1.6754e-03,  2.7869e-03,  1.8022e-03,  6.5846e-04,\n",
       "        -2.8961e-03,  7.1292e-04, -6.0671e-04, -3.3843e-03,  4.2672e-04,\n",
       "        -2.6801e-04,  2.7006e-03,  3.7393e-04, -1.5884e-06,  9.4714e-04,\n",
       "        -2.1190e-04, -1.2787e-03,  2.2849e-03,  1.1469e-03,  5.2262e-04,\n",
       "         1.6880e-03, -5.7188e-05, -1.1858e-03, -8.0548e-04, -5.2130e-05,\n",
       "         1.5018e-03, -3.1569e-03, -1.5300e-03, -2.9863e-03,  8.5885e-04,\n",
       "         7.6184e-04, -1.2801e-04,  1.6192e-03,  2.0990e-04, -1.8738e-03,\n",
       "        -1.5592e-03, -7.7176e-05,  9.5697e-04, -6.8943e-05,  1.2159e-03,\n",
       "        -1.2596e-04, -3.8160e-03,  4.8071e-04,  1.5715e-03, -2.3617e-03,\n",
       "         1.7820e-04,  4.8592e-04, -1.5564e-03, -1.0984e-04, -1.4019e-04,\n",
       "         2.7223e-04,  4.9297e-05,  5.3507e-04,  5.3987e-04,  2.5069e-03,\n",
       "         1.1186e-03, -4.1709e-03,  1.8977e-03,  6.4780e-04, -3.1663e-03,\n",
       "        -9.4930e-04,  1.0352e-03,  2.0868e-03, -3.6547e-04, -2.1761e-03,\n",
       "         1.3241e-03, -2.2028e-03,  1.2327e-03, -9.3723e-04,  2.8892e-03,\n",
       "         7.2784e-04, -3.7400e-03, -4.3711e-04,  1.7922e-03, -4.0121e-04,\n",
       "        -1.6632e-03, -7.3762e-04, -1.0603e-03, -3.7342e-04,  1.5506e-03,\n",
       "         2.7680e-03,  6.1885e-04, -2.6076e-03, -1.0259e-03,  1.3710e-03,\n",
       "        -1.4496e-04,  5.9461e-04, -8.3699e-04, -1.9363e-03,  3.4037e-03,\n",
       "         5.2825e-04, -5.8445e-04, -3.0194e-04,  1.0133e-04, -1.5699e-03,\n",
       "         1.8344e-03,  4.7073e-04, -2.2478e-04, -9.1063e-04,  3.3121e-04,\n",
       "         4.3994e-04, -6.0852e-04, -6.2461e-05, -1.1615e-03,  9.7975e-04,\n",
       "        -4.2783e-04,  1.1732e-03,  8.8793e-04, -1.8280e-04, -1.2328e-04,\n",
       "         3.4505e-04,  4.4158e-03, -2.0217e-04, -2.4527e-04, -1.9559e-04,\n",
       "         2.5969e-03,  5.0065e-04, -1.2200e-03, -5.8683e-04,  9.8124e-04,\n",
       "         1.4805e-03,  2.0585e-03, -1.5345e-03, -3.2819e-03,  2.2583e-03,\n",
       "         4.7169e-05,  3.0394e-03, -2.2886e-03,  2.5303e-04,  6.9976e-04,\n",
       "        -2.4201e-03,  3.2978e-03, -1.3181e-03,  1.7381e-04, -7.4742e-04,\n",
       "         6.6746e-04,  1.3211e-03,  8.3433e-04, -5.0607e-04,  2.8691e-04,\n",
       "        -8.1874e-05,  1.5794e-03, -3.5960e-04, -1.3686e-03, -2.2591e-05,\n",
       "        -6.5699e-05, -1.0243e-03, -7.4415e-04,  1.7821e-03, -1.0499e-05,\n",
       "         6.1990e-04, -4.8373e-04, -6.4391e-05, -8.3934e-04, -2.3203e-03,\n",
       "        -7.8956e-04,  2.2893e-05, -1.7872e-03, -1.5557e-03,  1.1288e-03,\n",
       "        -1.7643e-03,  1.3082e-03, -2.2686e-03,  1.5043e-04, -4.2071e-04,\n",
       "         2.4272e-03,  4.0125e-03, -3.4898e-03, -2.6825e-03,  5.0084e-05,\n",
       "         1.0428e-03,  8.9012e-04,  2.4029e-04, -9.7478e-04, -5.8557e-04,\n",
       "        -2.2160e-03,  2.5158e-04, -3.9082e-05,  1.5531e-03,  2.3779e-04,\n",
       "         2.5280e-03, -5.4046e-03, -1.6093e-03, -1.8058e-03,  6.6599e-04,\n",
       "        -2.5950e-05,  2.3898e-03,  7.2322e-05, -2.8331e-04, -7.3409e-04,\n",
       "         2.4147e-04, -8.7056e-05, -2.6014e-03,  1.1971e-03, -2.8099e-05,\n",
       "        -4.0789e-05, -3.4171e-04,  1.1813e-03, -7.3488e-04,  2.7986e-03,\n",
       "        -2.1043e-03, -1.9264e-03, -1.9728e-04, -9.9148e-04,  1.2596e-03,\n",
       "        -4.8896e-04, -9.0290e-04,  3.8326e-04,  2.8104e-03, -6.2565e-04,\n",
       "        -3.0162e-03, -8.6721e-04,  1.5466e-03, -1.1614e-03,  3.3045e-04,\n",
       "        -2.6354e-04, -3.2625e-05, -9.1887e-04,  1.9473e-04,  1.9965e-03,\n",
       "         5.3274e-04,  9.1101e-04, -6.8949e-04, -1.0085e-03, -1.4388e-04,\n",
       "         8.5304e-04, -6.2234e-07, -4.0955e-04, -2.8474e-05,  7.6850e-04,\n",
       "         8.3573e-04, -2.6748e-03, -6.7078e-04, -1.0557e-03,  5.8076e-04,\n",
       "        -1.5579e-03,  1.3615e-03, -1.1169e-03])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients[0][0] / sum(abs(i) for i in gradients[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dba08116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The quick brown fox jumps over the'\n",
      "Model's predicted next token: ' fence'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 2. Input text\n",
    "input_text = \"The quick brown fox jumps over the\"\n",
    "\n",
    "# 3. Tokenize and get model prediction\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()  # Get last token prediction\n",
    "\n",
    "# 4. Decode the predicted token\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "print(f\"Input: '{input_text}'\")\n",
    "print(f\"Model's predicted next token: '{predicted_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68cab1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Target token ' barking' not found in vocabulary. Using UNK ID (50256). Results might not be meaningful.\n",
      "Modified text: The quick brown fox jumps over the\n",
      "Original probability of ' barking': 0.0000003711\n",
      "Modified probability of ' barking': 0.0004909183\n",
      "\n",
      "Token changes: \n",
      "No change : The\n",
      "No change : Ġquick\n",
      "No change : Ġbrown\n",
      "No change : Ġfox\n",
      "No change : Ġjumps\n",
      "No change : Ġover\n",
      "No change : Ġthe\n"
     ]
    }
   ],
   "source": [
    "from inseq import load_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 1. Load model with gradient-based attribution\n",
    "model = load_model(\"gpt2\", \"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = model.model.get_input_embeddings()\n",
    "\n",
    "# 2. Define input and target output token\n",
    "input_text = \"The quick brown fox jumps over the\"\n",
    "target_token = \" barking\"  # Token we want to make more favorable\n",
    "\n",
    "# 3. Get token IDs\n",
    "inputs = model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "target_id = model.tokenizer.convert_tokens_to_ids(target_token)\n",
    "\n",
    "# # For optimizing toward a phrase:\n",
    "# target_ids = [model.tokenizer.convert_tokens_to_ids(t) for t in [\"lazy\", \"dog\"]]\n",
    "# target_logits = sum(outputs.logits[0, -len(target_ids):, target_ids].diag())\n",
    "\n",
    "if target_id == model.tokenizer.unk_token_id:\n",
    "     print(f\"Warning: Target token '{target_token}' not found in vocabulary. Using UNK ID ({target_id}). Results might not be meaningful.\")\n",
    "\n",
    "# 4. Perform forward pass with gradient tracking\n",
    "with torch.set_grad_enabled(True):\n",
    "    # Get embeddings and mark as requiring gradients\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    embeddings.retain_grad()\n",
    "    embeddings.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass through model\n",
    "    outputs = model.model(\n",
    "        inputs_embeds=embeddings,\n",
    "        attention_mask=inputs.attention_mask\n",
    "    )\n",
    "    \n",
    "    # Get logits for the target position (next token prediction)\n",
    "    target_position = -1  # Position after last input token\n",
    "    target_logit = outputs.logits[0, target_position, target_id]\n",
    "    \n",
    "    # Compute gradients\n",
    "    target_logit.backward()\n",
    "    gradients = embeddings.grad.clone()\n",
    "\n",
    "# # 5. Modify embeddings with gradients\n",
    "modified_embeddings = embeddings + gradients\n",
    "\n",
    "all_embeddings = embedding_layer.weight #Vocab Embeddings\n",
    "new_token_ids = []\n",
    "for i, emb in enumerate(modified_embeddings[0]) : \n",
    "    distances = F.cosine_similarity(emb, all_embeddings, dim=-1)\n",
    "    new_id = torch.argmax(distances).item()\n",
    "    new_token_ids.append(new_id)\n",
    "\n",
    "#Decode the new tokens\n",
    "new_tokens = model.tokenizer.convert_ids_to_tokens(new_token_ids)\n",
    "new_text = model.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "print(f\"Modified text: {new_text}\")\n",
    "\n",
    "# 8 Compare Probabilities\n",
    "with torch.no_grad():\n",
    "    modified_outputs = model.model(\n",
    "        inputs_embeds=modified_embeddings,\n",
    "        attention_mask=inputs.attention_mask\n",
    "    )\n",
    "    original_probs = torch.softmax(outputs.logits[0, target_position], dim=-1)\n",
    "    modified_probs = torch.softmax(modified_outputs.logits[0, target_position], dim=-1)\n",
    "\n",
    "print(f\"Original probability of '{target_token}': {original_probs[target_id].item():.10f}\")\n",
    "print(f\"Modified probability of '{target_token}': {modified_probs[target_id].item():.10f}\")\n",
    "\n",
    "\n",
    "# Show token substitutions\n",
    "print(\"\\nToken changes: \")\n",
    "for orig, new in zip(model.tokenizer.tokenize(input_text), new_tokens):\n",
    "    if orig != new: \n",
    "        print(f\"{orig} -> {new}\")\n",
    "    else : \n",
    "        print(\"No change :\", orig)\n",
    "# # Optional: Visualize gradient magnitudes\n",
    "# gradient_magnitudes = torch.norm(gradients, dim=-1).squeeze()\n",
    "# print(\"\\nGradient magnitudes per input token:\")\n",
    "# for token, magnitude in zip(model.tokenizer.convert_ids_to_tokens(inputs.input_ids[0]), gradient_magnitudes):\n",
    "#     print(f\"{token:>10}: {magnitude.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43bad54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db1ea9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad1ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8473b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de53020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b37fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b429d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1dcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf465fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\XAI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import inseq\n",
    "from inseq import load_model\n",
    "# from inseq.models import InseqModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae382fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'\n",
    "INPUT_TEXT = \"The quick fox jumps over the lazy \"\n",
    "TARGET_OUTPUT_TOKEN = \"dog\"\n",
    "TARGET_OUTPUT_POSITION = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d43c10b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\XAI\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"gpt2\", \"saliency\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b6338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token = model.tokenizer(INPUT_TEXT, return_tensors='pt')\n",
    "input_ids = input_token[\"input_ids\"].to(model.device)\n",
    "attention_mask = input_token[\"attention_mask\"].to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19e04383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token 'dog' corresponds to ID: 9703\n"
     ]
    }
   ],
   "source": [
    "target_token_id = model.tokenizer.convert_tokens_to_ids(TARGET_OUTPUT_TOKEN)\n",
    "if target_token_id == model.tokenizer.unk_token_id: \n",
    "    print(f\"Warning: Target token '{TARGET_OUTPUT_TOKEN}' not found in vocabulary. Using UNK ID ({target_token_id}). Results might not be meaningful.\")\n",
    "\n",
    "print(f\"Target token '{TARGET_OUTPUT_TOKEN}' corresponds to ID: {target_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62882d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e051c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input embeddings\n",
    "# with torch.set_grad_enabled(True) : \n",
    "original_embeddings = model.model.get_input_embeddings()(input_ids)\n",
    "original_embeddings.clone().detach().requires_grad_(True)\n",
    "\n",
    "if hasattr(model.model, 'encoder') and hasattr(model.model, 'decoder'): \n",
    "    print(\"Detected Seq2Seq model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
